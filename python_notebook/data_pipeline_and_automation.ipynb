{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In a real-world production environment, data arrives continuously from multiple sources - not just one static CSV.\n",
        "\n",
        "This section defines a reusable pipeline function that automates the process of loading new session data, preprocessing it using the saved\n",
        "pipeline, generating predictions with the trained model, and saving the results.\n",
        "\n",
        "The function below simulates what a real e-commerce prediction service would do daily:\n",
        "    \n",
        "- Detect and load new data\n",
        "\n",
        "- Apply preprocessing transformations\n",
        "\n",
        "- Generate model predictions\n",
        "\n",
        "- Save results to a timestamped file or database"
      ],
      "metadata": {
        "id": "oqaZJooWVUh9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gxxq47uRUxza"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Automation Pipeline Function\n",
        "def run_cart_abandonment_pipeline(\n",
        "    model_path=\"best_svm.pkl\",\n",
        "    preprocessor_path=\"preprocessor.pkl\",\n",
        "    new_data_path=\"new_sessions.csv\",\n",
        "    output_dir=\"predictions_output\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Automates the prediction workflow for new session data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_path : str\n",
        "        Path to the saved trained model (.pkl file).\n",
        "    preprocessor_path : str\n",
        "        Path to the saved preprocessing pipeline (.pkl file).\n",
        "    new_data_path : str\n",
        "        Path to the new session CSV file (simulated daily data).\n",
        "    output_dir : str\n",
        "        Directory to save prediction output files.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        File path of the saved predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nStarting Cart Abandonment Automation Pipeline\")\n",
        "\n",
        "\n",
        "    # Load preprocessor and model\n",
        "    if not os.path.exists(model_path) or not os.path.exists(preprocessor_path):\n",
        "        raise FileNotFoundError(\"Model or preprocessor file not found. Check file paths.\")\n",
        "\n",
        "    model = joblib.load(model_path)\n",
        "    preprocessor = joblib.load(preprocessor_path)\n",
        "    print(\"Loaded model and preprocessing pipeline successfully\")\n",
        "\n",
        "\n",
        "    # Load new session data\n",
        "    if not os.path.exists(new_data_path):\n",
        "        raise FileNotFoundError(f\"New data file not found at: {new_data_path}\")\n",
        "\n",
        "    new_df = pd.read_csv(new_data_path)\n",
        "    print(f\"Loaded {len(new_df)} new session records from '{new_data_path}'\")\n",
        "\n",
        "    # Preprocess the data\n",
        "    X_new_prepared = preprocessor.transform(new_df)\n",
        "\n",
        "    # Get predictions and probabilities\n",
        "    y_pred = model.predict(X_new_prepared)\n",
        "    y_proba = model.predict_proba(X_new_prepared)[:, 1]\n",
        "\n",
        "\n",
        "    # Combine with original data\n",
        "    predictions_df = new_df.copy()\n",
        "    predictions_df[\"predicted_class\"] = y_pred\n",
        "    predictions_df[\"abandonment_risk\"] = y_proba\n",
        "\n",
        "\n",
        "    # Save predictions\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_path = os.path.join(output_dir, f\"predictions_{timestamp}.csv\")\n",
        "    predictions_df.to_csv(output_path, index=False)\n",
        "    print(f\"Predictions saved to: {output_path}\")\n",
        "\n",
        "    print(\"Automation pipeline completed successfully.\\n\")\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "XMUwARX4WDAT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be triggered manually or via a scheduler later."
      ],
      "metadata": {
        "id": "O5jlkBGHepJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Pipeline\n",
        "run_cart_abandonment_pipeline()"
      ],
      "metadata": {
        "id": "ICAbR-QSHPWs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}